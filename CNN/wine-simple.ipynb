{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras import models,layers\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/wine.data'\n",
    "df = pd.read_csv(data_path, sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        1     2     3     4    5     6     7     8     9     10    11    12  \\\n",
       " 95   12.47  1.52  2.20  19.0  162  2.50  2.27  0.32  3.28  2.60  1.16  2.63   \n",
       " 91   12.00  1.51  2.42  22.0   86  1.45  1.25  0.50  1.63  3.60  1.05  2.65   \n",
       " 24   13.50  1.81  2.61  20.0   96  2.53  2.61  0.28  1.66  3.52  1.12  3.82   \n",
       " 109  11.61  1.35  2.70  20.0   94  2.74  2.92  0.29  2.49  2.65  0.96  3.26   \n",
       " 121  11.56  2.05  3.23  28.5  119  3.18  5.08  0.47  1.87  6.00  0.93  3.69   \n",
       " ..     ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...   ...   \n",
       " 113  11.41  0.74  2.50  21.0   88  2.48  2.01  0.42  1.44  3.08  1.10  2.31   \n",
       " 64   12.17  1.45  2.53  19.0  104  1.89  1.75  0.45  1.03  2.95  1.45  2.23   \n",
       " 15   13.63  1.81  2.70  17.2  112  2.85  2.91  0.30  1.46  7.30  1.28  2.88   \n",
       " 125  12.07  2.16  2.17  21.0   85  2.60  2.65  0.37  1.35  2.76  0.86  3.28   \n",
       " 9    13.86  1.35  2.27  16.0   98  2.98  3.15  0.22  1.85  7.22  1.01  3.55   \n",
       " \n",
       "        13  \n",
       " 95    937  \n",
       " 91    450  \n",
       " 24    845  \n",
       " 109   680  \n",
       " 121   465  \n",
       " ..    ...  \n",
       " 113   434  \n",
       " 64    355  \n",
       " 15   1310  \n",
       " 125   378  \n",
       " 9    1045  \n",
       " \n",
       " [142 rows x 13 columns],\n",
       "         1     2     3     4    5     6     7     8     9         10     11  \\\n",
       " 69   12.21  1.19  1.75  16.8  151  1.85  1.28  0.14  2.50  2.850000  1.280   \n",
       " 103  11.82  1.72  1.88  19.5   86  2.50  1.64  0.37  1.42  2.060000  0.940   \n",
       " 20   14.06  1.63  2.28  16.0  126  3.00  3.17  0.24  2.10  5.650000  1.090   \n",
       " 116  11.82  1.47  1.99  20.8   86  1.98  1.60  0.30  1.53  1.950000  0.950   \n",
       " 56   14.22  1.70  2.30  16.3  118  3.20  3.00  0.26  2.03  6.380000  0.940   \n",
       " 97   12.29  1.41  1.98  16.0   85  2.55  2.50  0.29  1.77  2.900000  1.230   \n",
       " 117  12.42  1.61  2.19  22.5  108  2.00  2.09  0.34  1.61  2.060000  1.060   \n",
       " 39   14.22  3.99  2.51  13.2  128  3.00  3.04  0.20  2.08  5.100000  0.890   \n",
       " 172  14.16  2.51  2.48  20.0   91  1.68  0.70  0.44  1.24  9.700000  0.620   \n",
       " 10   14.10  2.16  2.30  18.0  105  2.95  3.32  0.22  2.38  5.750000  1.250   \n",
       " 19   13.64  3.10  2.56  15.2  116  2.70  3.03  0.17  1.66  5.100000  0.960   \n",
       " 66   13.11  1.01  1.70  15.0   78  2.98  3.18  0.26  2.28  5.300000  1.120   \n",
       " 55   13.56  1.73  2.46  20.5  116  2.96  2.78  0.20  2.45  6.250000  0.980   \n",
       " 61   12.64  1.36  2.02  16.8  100  2.02  1.41  0.53  0.62  5.750000  0.980   \n",
       " 102  12.34  2.45  2.46  21.0   98  2.56  2.11  0.34  1.31  2.800000  0.800   \n",
       " 59   12.37  0.94  1.36  10.6   88  1.98  0.57  0.28  0.42  1.950000  1.050   \n",
       " 83   13.05  3.86  2.32  22.5   85  1.65  1.59  0.61  1.62  4.800000  0.840   \n",
       " 78   12.33  0.99  1.95  14.8  136  1.90  1.85  0.35  2.76  3.400000  1.060   \n",
       " 110  11.46  3.74  1.82  19.5  107  3.18  2.58  0.24  3.58  2.900000  0.750   \n",
       " 163  12.96  3.45  2.35  18.5  106  1.39  0.70  0.40  0.94  5.280000  0.680   \n",
       " 1    13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.380000  1.050   \n",
       " 160  12.36  3.83  2.38  21.0   88  2.30  0.92  0.50  1.04  7.650000  0.560   \n",
       " 47   13.90  1.68  2.12  16.0  101  3.10  3.39  0.21  2.14  6.100000  0.910   \n",
       " 49   13.94  1.73  2.27  17.4  108  2.88  3.54  0.32  2.08  8.900000  1.120   \n",
       " 67   12.37  1.17  1.92  19.6   78  2.11  2.00  0.27  1.04  4.680000  1.120   \n",
       " 136  12.25  4.72  2.54  21.0   89  1.38  0.47  0.53  0.80  3.850000  0.750   \n",
       " 68   13.34  0.94  2.36  17.0  110  2.53  1.30  0.55  0.42  3.170000  1.020   \n",
       " 133  12.70  3.55  2.36  21.5  106  1.70  1.20  0.17  0.84  5.000000  0.780   \n",
       " 60   12.33  1.10  2.28  16.0  101  2.05  1.09  0.63  0.41  3.270000  1.250   \n",
       " 76   13.03  0.90  1.71  16.0   86  1.95  2.03  0.24  1.46  4.600000  1.190   \n",
       " 171  12.77  2.39  2.28  19.5   86  1.39  0.51  0.48  0.64  9.899999  0.570   \n",
       " 170  12.20  3.03  2.32  19.0   96  1.25  0.49  0.40  0.73  5.500000  0.660   \n",
       " 70   12.29  1.61  2.21  20.4  103  1.10  1.02  0.37  1.46  3.050000  0.906   \n",
       " 168  13.58  2.58  2.69  24.5  105  1.55  0.84  0.39  1.54  8.660000  0.740   \n",
       " 124  11.87  4.31  2.39  21.0   82  2.86  3.03  0.21  2.91  2.800000  0.750   \n",
       " 26   13.39  1.77  2.62  16.1   93  2.85  2.94  0.34  1.45  4.800000  0.920   \n",
       " \n",
       "        12    13  \n",
       " 69   3.07   718  \n",
       " 103  2.44   415  \n",
       " 20   3.71   780  \n",
       " 116  3.33   495  \n",
       " 56   3.31   970  \n",
       " 97   2.74   428  \n",
       " 117  2.96   345  \n",
       " 39   3.53   760  \n",
       " 172  1.71   660  \n",
       " 10   3.17  1510  \n",
       " 19   3.36   845  \n",
       " 66   3.18   502  \n",
       " 55   3.03  1120  \n",
       " 61   1.59   450  \n",
       " 102  3.38   438  \n",
       " 59   1.82   520  \n",
       " 83   2.01   515  \n",
       " 78   2.31   750  \n",
       " 110  2.81   562  \n",
       " 163  1.75   675  \n",
       " 1    3.40  1050  \n",
       " 160  1.58   520  \n",
       " 47   3.33   985  \n",
       " 49   3.10  1260  \n",
       " 67   3.48   510  \n",
       " 136  1.27   720  \n",
       " 68   1.93   750  \n",
       " 133  1.29   600  \n",
       " 60   1.67   680  \n",
       " 76   2.48   392  \n",
       " 171  1.63   470  \n",
       " 170  1.83   510  \n",
       " 70   1.82   870  \n",
       " 168  1.80   750  \n",
       " 124  3.64   380  \n",
       " 26   3.22  1195  ,\n",
       " 95     2\n",
       " 91     2\n",
       " 24     1\n",
       " 109    2\n",
       " 121    2\n",
       "       ..\n",
       " 113    2\n",
       " 64     2\n",
       " 15     1\n",
       " 125    2\n",
       " 9      1\n",
       " Name: 0, Length: 142, dtype: int64,\n",
       " 69     2\n",
       " 103    2\n",
       " 20     1\n",
       " 116    2\n",
       " 56     1\n",
       " 97     2\n",
       " 117    2\n",
       " 39     1\n",
       " 172    3\n",
       " 10     1\n",
       " 19     1\n",
       " 66     2\n",
       " 55     1\n",
       " 61     2\n",
       " 102    2\n",
       " 59     2\n",
       " 83     2\n",
       " 78     2\n",
       " 110    2\n",
       " 163    3\n",
       " 1      1\n",
       " 160    3\n",
       " 47     1\n",
       " 49     1\n",
       " 67     2\n",
       " 136    3\n",
       " 68     2\n",
       " 133    3\n",
       " 60     2\n",
       " 76     2\n",
       " 171    3\n",
       " 170    3\n",
       " 70     2\n",
       " 168    3\n",
       " 124    2\n",
       " 26     1\n",
       " Name: 0, dtype: int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[0]  # Target column\n",
    "X = df.drop(0, axis=1)  # Feature columns\n",
    "\n",
    "# Splitting the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\OneDrive\\Desktop\\AT\\MS(R)\\CNN\\.conda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(100,activation= 'relu', input_shape = (13,)))\n",
    "network.add(layers.Dense(4,activation= 'softmax', input_shape = (13)))\n",
    "network.compile(optimizer=\"rmsprop\", loss = \"categorical_crossentropy\", metrics= [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3414 - loss: 92.3889   \n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3614 - loss: 37.9735 \n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2678 - loss: 9.0668 \n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5347 - loss: 3.6221 \n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.3256 - loss: 3.5497 \n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4604 - loss: 2.4549 \n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5841 - loss: 1.8879 \n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4519 - loss: 2.4802 \n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5764 - loss: 2.4732 \n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4776 - loss: 3.0316 \n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6106 - loss: 1.4414 \n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5483 - loss: 2.8650 \n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5315 - loss: 2.0436 \n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6202 - loss: 1.2795 \n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5477 - loss: 1.8377 \n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6198 - loss: 1.9583 \n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4402 - loss: 4.8104 \n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5938 - loss: 1.6903 \n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4772 - loss: 3.6667 \n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6284 - loss: 1.5494 \n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6352 - loss: 1.9620 \n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5542 - loss: 1.9501 \n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5655 - loss: 2.4080 \n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6120 - loss: 1.9968 \n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6690 - loss: 1.1952 \n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5946 - loss: 3.2582 \n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6354 - loss: 1.3526 \n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6173 - loss: 1.7661 \n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5657 - loss: 2.3735 \n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5985 - loss: 2.9431 \n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5764 - loss: 1.8780 \n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6605 - loss: 1.4627 \n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6943 - loss: 1.4549 \n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6743 - loss: 1.3584 \n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7067 - loss: 1.3188 \n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6308 - loss: 1.9198 \n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5888 - loss: 2.2039 \n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5776 - loss: 2.2456 \n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6696 - loss: 1.9911 \n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6405 - loss: 1.3535 \n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7340 - loss: 1.1639 \n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5011 - loss: 2.5138 \n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8186 - loss: 0.6009 \n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5895 - loss: 2.2683 \n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6780 - loss: 0.9828 \n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7084 - loss: 1.0977 \n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6116 - loss: 2.7350 \n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7941 - loss: 0.5654 \n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6105 - loss: 1.9736 \n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6803 - loss: 1.6063 \n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6744 - loss: 1.3756 \n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6598 - loss: 1.1287 \n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5319 - loss: 3.1922 \n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6903 - loss: 1.0691 \n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6861 - loss: 1.2494 \n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5981 - loss: 2.2688 \n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6632 - loss: 1.4180 \n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7362 - loss: 1.0014 \n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5798 - loss: 2.0220 \n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5613 - loss: 2.0984 \n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5994 - loss: 1.7243 \n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5375 - loss: 2.4949 \n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7515 - loss: 0.8253 \n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.8532 \n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6481 - loss: 1.3057 \n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5846 - loss: 1.4358 \n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6630 - loss: 1.4405 \n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6676 - loss: 1.7043 \n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8129 - loss: 0.5617 \n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 1.1898 \n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6600 - loss: 1.0953 \n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5861 - loss: 2.3555 \n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6888 - loss: 2.1233 \n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 1.8442 \n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7317 - loss: 1.0329 \n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.7239 \n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6745 - loss: 1.8329 \n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6929 - loss: 1.3446 \n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5317 - loss: 2.6977 \n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7490 - loss: 0.7969 \n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8241 - loss: 0.6101 \n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6696 - loss: 1.5212 \n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5652 - loss: 1.6166 \n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7052 - loss: 1.4189 \n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8197 - loss: 0.5378 \n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5851 - loss: 2.8985 \n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7555 - loss: 1.4484 \n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7315 - loss: 0.8309 \n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7039 - loss: 1.1001 \n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7303 - loss: 1.2272 \n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6654 - loss: 1.5419 \n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 1.2820 \n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6530 - loss: 1.6356 \n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9211 - loss: 0.2928 \n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5701 - loss: 2.7103 \n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6342 - loss: 2.2212 \n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9166 - loss: 0.3168 \n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7378 - loss: 0.8370 \n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7321 - loss: 1.0650 \n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7383 - loss: 1.0690 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22c61558110>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(X_train, train_labels, epochs=100, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C616AEB60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 46ms/stepWARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000022C616AEB60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6736 - loss: 2.9999  \n",
      "0.6666666865348816 3.0230956077575684\n"
     ]
    }
   ],
   "source": [
    "predicted = network.predict(X_test)\n",
    "test_loss, test_acc = network.evaluate(X_test, test_labels)\n",
    "print(test_acc, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\OneDrive\\Desktop\\AT\\MS(R)\\CNN\\.conda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.3875 - loss: 1.0479\n",
      "Epoch 2/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6666 - loss: 0.9271 \n",
      "Epoch 3/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9483 - loss: 0.8143\n",
      "Epoch 4/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8886 - loss: 0.7590 \n",
      "Epoch 5/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9311 - loss: 0.6818  \n",
      "Epoch 6/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9531 - loss: 0.6275\n",
      "Epoch 7/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9408 - loss: 0.5896  \n",
      "Epoch 8/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9137 - loss: 0.5486 \n",
      "Epoch 9/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9431 - loss: 0.5113  \n",
      "Epoch 10/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9481 - loss: 0.4799  \n",
      "Epoch 11/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9160 - loss: 0.4570 \n",
      "Epoch 12/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8978 - loss: 0.4431  \n",
      "Epoch 13/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9177 - loss: 0.4274  \n",
      "Epoch 14/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9623 - loss: 0.3664  \n",
      "Epoch 15/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9581 - loss: 0.3531  \n",
      "Epoch 16/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9209 - loss: 0.3493 \n",
      "Epoch 17/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9555 - loss: 0.3218  \n",
      "Epoch 18/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9680 - loss: 0.3061  \n",
      "Epoch 19/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9606 - loss: 0.2781  \n",
      "Epoch 20/20\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9531 - loss: 0.2650 \n",
      "WARNING:tensorflow:6 out of the last 15 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x0000022C670577E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8947 - loss: 0.2800 \n",
      "Test Accuracy: 0.8888888955116272 \n",
      "Test Loss: 0.2786354124546051\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19  0  0]\n",
      " [ 5 15  1]\n",
      " [ 0  0 14]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "file_name = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "X = pd.read_csv(file_name, sep=\",\", header=None)\n",
    "\n",
    "# Separate target and features\n",
    "target = np.asarray(X[0]) - 1  # Adjust target indices to start from 0\n",
    "data = np.asarray(X.drop(0, axis=1))\n",
    "\n",
    "# Determine the number of unique classes\n",
    "num_classes = len(np.unique(target))\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to categorical\n",
    "train_labels = to_categorical(y_train, num_classes=num_classes)\n",
    "test_labels = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Create the network\n",
    "network = Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(13,)))\n",
    "network.add(layers.Dense(num_classes, activation='softmax'))  # Match the number of output neurons to the number of classes\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the network\n",
    "network.fit(X_train, train_labels, epochs=20, batch_size=40)\n",
    "\n",
    "# Evaluate the network\n",
    "test_loss, test_acc = network.evaluate(X_test, test_labels)\n",
    "print('Test Accuracy:', test_acc, '\\nTest Loss:', test_loss)\n",
    "\n",
    "# Predict and prepare for confusion matrix\n",
    "y_pred_prob = network.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)  # Correct way to extract predicted class labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Visualizing the confusion matrix using seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=np.unique(target), yticklabels=np.unique(target))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
