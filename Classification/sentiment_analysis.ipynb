{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Multinomial Naive Bayes\n",
    "\n",
    "This notebook demonstrates the application of Multinomial Naive Bayes for sentiment analysis on a dataset comprising 1.6 million tweets. The dataset, known as Sentiment140, contains tweets annotated with sentiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The Sentiment140 dataset includes the following columns:\n",
    "\n",
    "- **target**: the polarity of the tweet (0 = negative, 4 = positive)\n",
    "- **ids**: The id of the tweet (2087)\n",
    "- **date**: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- **flag**: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- **user**: the user that tweeted (robotickilldozr)\n",
    "- **text**: the text of the tweet (Lyx is cool)\n",
    "\n",
    "The objective is to predict the sentiment of the tweets as positive or negative using the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('sample_dataset.csv', names=['target', 'ids', 'date', 'flag', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>target</td>\n",
       "      <td>ids</td>\n",
       "      <td>date</td>\n",
       "      <td>flag</td>\n",
       "      <td>user</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734779.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2264631528</td>\n",
       "      <td>Sun Jun 21 04:28:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>andychong9</td>\n",
       "      <td>dad having fever again.. not looking too good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632647.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2232741622</td>\n",
       "      <td>Thu Jun 18 20:19:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SLeepdepD</td>\n",
       "      <td>@judahgabriel i wish i had that much to say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337706.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2014269923</td>\n",
       "      <td>Wed Jun 03 00:59:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BlackCat_Saya</td>\n",
       "      <td>@rohan_01 you know..it's really sad that u kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465228.0</th>\n",
       "      <td>0</td>\n",
       "      <td>2175302784</td>\n",
       "      <td>Mon Jun 15 00:36:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sawarahh</td>\n",
       "      <td>@cathicks i don't get it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target         ids                          date      flag  \\\n",
       "NaN       target         ids                          date      flag   \n",
       "734779.0       0  2264631528  Sun Jun 21 04:28:40 PDT 2009  NO_QUERY   \n",
       "632647.0       0  2232741622  Thu Jun 18 20:19:48 PDT 2009  NO_QUERY   \n",
       "337706.0       0  2014269923  Wed Jun 03 00:59:10 PDT 2009  NO_QUERY   \n",
       "465228.0       0  2175302784  Mon Jun 15 00:36:56 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \n",
       "NaN                user                                               text  \n",
       "734779.0     andychong9     dad having fever again.. not looking too good   \n",
       "632647.0      SLeepdepD       @judahgabriel i wish i had that much to say   \n",
       "337706.0  BlackCat_Saya  @rohan_01 you know..it's really sad that u kno...  \n",
       "465228.0       sawarahh                         @cathicks i don't get it.   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"text dad having fever again.. not looking too good  @judahgabriel i wish i had that much to say  @rohan_01 you know..it's really sad that u know that ur classmate don't care about you..        sad memories t.t @cathicks i don't get it.  @dougsky i will have a look when i get home!  @mikegentile hey stud. go figure you come to the land of whores when i am gone in the bahamas  @alyxayer  and @richbello ... hey it was a player from akron!   i hope you guys thought of me, haha, it's okay i know you didn't! @joejgirl2009 oh... cool... standing out...  eaten way too much junk food. feel as if i'm about to explode! not good  needs help from a wordpress theme expert  http://plurk.com/p/ywsk3 a great hard training weekend is over.  a couple days of rest and lets do it again!  lots of computer time to put in now  @faketragedycom i know  @teufl0302 @stevebrunton sweeties, i gotta go... having dinner and eating lots of cake with my mom  school over in 4 more days no more teachers!! grad on wednesday pumped// crystal palace tomorrow going to be fun  @unworthysaint are you having fun?  hey debby or any body else...can sum1 talk 2 me right now???  ow, i can't move my neck or my back   hurts too much and i don't know why d: @richdirtygirl  slides show with voice i hope  @f_nadzirah cant sleep,go online! hahaha @bobbyden and @faidchong misses you too  @sherrieshepherd nothing like tradition handed down  @halestormrocks what's up y'all! we've been rockin ur stuff here in okc at da katt! tune in at www.katt.com anytime!  @xxandip love that bag......!!! enjoy the gym and your pineapple  @innerwizard oh thanks so much i love your sight i favorited to go over it.  so positve  @beesknees42 set the clock ahead two hours and make him think its later  home from work... was missing my maddie bug   now off to put clothes away and make our bed!!! maybe sleep in it tonight!!! @johnhood sadly, bruce wayne wld never rock up to where i am! re: macbooks. nup. i'm grunge geek. dell xps m1330 + advent 4211c. cheap!  @alyciaolson hi sweetheart! glad you found me on twitter! whazzup!? hope you are sleeping well and &quot;tweet&quot; me again tomorrow! love you!  watching a progress bar go across the screen. very, very dull and tedious.  @veropperez awwww my poor muffins i had to make those and i didnt make them for the cat he took the whole freakin bag and ran away  aw man @chriscuzzy mermaids are almost extinct and should not be kept in captivity. and that's disney's fault! or so i heard!  bay to breakers in sf today... oh, how i miss those crazy days! can't wait to see the pics my friends will post  looks like i won't be getting operated b4 my album release   gonna surprise my pops at work and have lunch with him! happy late dad's day! @dannywhitehouse that last message wasn't a dig  &quot;binthia&quot; is serious now. she hot really quiet and looks mad tired. kinda feel bad cuz she had to drive after only sleeping a few hours  *bursts through the twitter doors* hellooooooooo!   @hazemtaji did you buy a ps3? my xbox360 got the rrod  @gavinosborn yeah, i did ask for him to be banned  saw walk the line tonight.never heard much cash before.i could definitely appreciate this.why do i only &quot;discover&quot; artists after they die  been working hard. london had glorious weather, came back to kent and it was raining  @pwbbounce uh oh  but is that not good? its not got a hidden meaning or anything that i dont know about has it? @pjraval they searched my person, too. one grabbed my stomach under my sweater. that's what i get for wearing a sweater in snowy weather.  my brother must hate me. he didn't answer his phone  and he didn't call me back. either that or he's sleeping or at work. ;) i guess alls well that ends well...i'm still single for the record lol. leaving cleveland will be bittersweet to  jean michel jarre concert tomorrow  gotta work 7-3 before though  @missdangermouse hell yes!  where have you been, man?  you're never on aim anymore.  is riding her horse!! ahh i missed him soo much!!!!  p.s. go see the star trek movie asap! @blokeslib wow... i was just thinking of the time difference between us! you've already lived my tomorrow  reading a few chapters of choke, sleep, work. i still feel like shit.  @divacandicem - candice i dont mean to bother you but im really sad that u got released!  @rerunaway follow me !!  @victoria_tweed opted for a corona - it was a mistake. my headache just got a million times worse  right, off to work  only 5 hours to go until i'm free xd owwwwwww damn my brother smacked my back and i have a slight sun burn on my back lets jsut say i got even i smacked him in the face  spent evening looking after suki doggy shitting herself everywhere, falling off the decking into ditch and whining &amp; running into things  im in da library, noelle and kayla and connie just left. should i? maybe cuz im really bored. noone 2 chat and laugh with.  wooo chix rice!  @blipfish re: current jam. @foxvox you nerd  sitting at work, waiting for the receptionist to get off lunch so i can go to my dental    i hate it! @ruidelgado i'm really looking into it as soon as it finishes.  saramago played by hollywood and having great reviews, that's a must see!! i.....want to.......go........home........  @justkaty i blame josh &amp; bff ..she was sick earlier &amp; so was josh  @amytheallen no... 19  i'd assume they'd check if i actually did win something @katemaxwell not wrong in the slightest. that's a true broad.  i would also like to make out with her, to suck the smoke from her lungs. @prateekgupta i went n have now returned  @trucco905 yeah i have these extra strength things my mum takes for her leg! and no chemist will be open when i go... god!  @mashupfacts u listed some great mashup artists there  @icedelosreyes don't underestimate my flexibility =p but my balance, hmmm... not so good. sama ka? sa valero lang siya  @jimmietryon my 62-yo neighbor uses linux for simple printing, email, web. no cli needed. i use the cli 4 shortcuts, mostly.  @lenartr i use y!pipes to personalize my rss feeds; but sure, this would be a good option for &quot;normal&quot; users  @girltalknyc love ur show and ur name.  just joined checking it out  @foxandfriends  gmornin to you guys from chicago  staring at tweezers ... my life is so exciting  a lot of people are auditioning for millie! exciting  i hope this summer will be as good as last. *twisted.. gahh studying is making screw up my commercial  @ivebeenabadboy lol thank you.  one month ago i was the most happiest person.. n now.. ijustwannadie  omfg why am i working right now?  @cksolutions  i guess because they still don't have basic stuff like social bookmarking. can olny share by email. still love picasa   à¸\\x94à¸¹à¹?à¸\\x9aà¹\\x84à¸\\x95à¹\\x8b hitech http://vibejournal.com/jessica_art/ needs a new home  burger king = love  am i the only one that puts fries in my burger and it the crust around it first? people, are looking at me weird :| 3 cheers for losing the cell phone charger cable  @xpowxbangxboomx dammiitt!! i wish i had mtv  out at dinner with the family  once the concert stories start rollin in i might have to go into hibernation  it's official....veggies taste best hot off the grill in the summer!  i am not poorly sick like @loobydo but i am not a happy bunny - i ache  wtf is going on with the chiefs? getting smashed at the 1/2 way point of the s14 final  @anniegxxx can't think of him that way any more  or at least i try not to lol @cause4conceit and i luv it!! but i'm very hurt that only sam goody had the album in my city and they sold out!!  @dustyedwards mornin'  @xnausikaax oh no! where did u order from? that's horrible  grounded  on the plane 2 spain  happy days. @plainbananas italian ice cream? ohhhh.... want that with my cake!  @babiibecca yeah idk im supposed to go back to escuela and do something but idk how long itl take. ill let chu know tho  #followfriday @ryanhurst @disklabs - both computer forensic based tweeters, who also tweet socially. intereting and amusing  @undertheground  i am craving for japanese food \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = ' '.join(df['text'].values)\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = all_text.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "tf_idf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>19</th>\n",
       "      <th>4211c</th>\n",
       "      <th>62</th>\n",
       "      <th>about</th>\n",
       "      <th>ache</th>\n",
       "      <th>across</th>\n",
       "      <th>actually</th>\n",
       "      <th>advent</th>\n",
       "      <th>after</th>\n",
       "      <th>again</th>\n",
       "      <th>...</th>\n",
       "      <th>xps</th>\n",
       "      <th>xxandip</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yes</th>\n",
       "      <th>yo</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>ywsk3</th>\n",
       "      <th>¹à¹</th>\n",
       "      <th>à¹</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070691</td>\n",
       "      <td>0.141382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 706 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     19  4211c   62     about      ache  across  actually  advent  after  \\\n",
       "0   0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "1   0.0    0.0  0.0  0.192474  0.000000     0.0       0.0     0.0    0.0   \n",
       "2   0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "3   0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "4   0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "..  ...    ...  ...       ...       ...     ...       ...     ...    ...   \n",
       "62  0.0    0.0  0.0  0.000000  0.070691     0.0       0.0     0.0    0.0   \n",
       "63  0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "64  0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "65  0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "66  0.0    0.0  0.0  0.000000  0.000000     0.0       0.0     0.0    0.0   \n",
       "\n",
       "       again  ...  xps  xxandip      yeah  yes   yo       you  your  ywsk3  \\\n",
       "0   0.410152  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "1   0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.266583   0.0    0.0   \n",
       "2   0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "3   0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "4   0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.166741   0.0    0.0   \n",
       "..       ...  ...  ...      ...       ...  ...  ...       ...   ...    ...   \n",
       "62  0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "63  0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "64  0.000000  ...  0.0      0.0  0.194067  0.0  0.0  0.000000   0.0    0.0   \n",
       "65  0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "66  0.000000  ...  0.0      0.0  0.000000  0.0  0.0  0.000000   0.0    0.0   \n",
       "\n",
       "         ¹à¹        à¹  \n",
       "0   0.000000  0.000000  \n",
       "1   0.000000  0.000000  \n",
       "2   0.000000  0.000000  \n",
       "3   0.000000  0.000000  \n",
       "4   0.000000  0.000000  \n",
       "..       ...       ...  \n",
       "62  0.070691  0.141382  \n",
       "63  0.000000  0.000000  \n",
       "64  0.000000  0.000000  \n",
       "65  0.000000  0.000000  \n",
       "66  0.000000  0.000000  \n",
       "\n",
       "[67 rows x 706 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We use CountVectorizer to convert text data into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "# Fitting and transforming the text data\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['target']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Multinomial Naive Bayes\n",
    "\n",
    "Now, we apply the Multinomial Naive Bayes algorithm to predict the sentiment of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6190476190476191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.82      0.72        11\n",
      "           4       0.57      0.44      0.50         9\n",
      "      target       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62        21\n",
      "   macro avg       0.40      0.42      0.41        21\n",
      "weighted avg       0.58      0.62      0.59        21\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\abhay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\abhay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Training the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
