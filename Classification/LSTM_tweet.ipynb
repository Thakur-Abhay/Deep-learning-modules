{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBybPxeLjpxS",
        "outputId": "b35bdcbc-bbb7-48a6-ba55-56e7ddc4e9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3/3 [==============================] - 3s 351ms/step - loss: 0.6511 - accuracy: 0.0000e+00 - val_loss: 0.6469 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "3/3 [==============================] - ETA: 0s - loss: 0.5910 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r3/3 [==============================] - 0s 76ms/step - loss: 0.5910 - accuracy: 0.0000e+00 - val_loss: 0.6095 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.5141 - accuracy: 0.0000e+00 - val_loss: 0.5606 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "3/3 [==============================] - 0s 79ms/step - loss: 0.4244 - accuracy: 0.0000e+00 - val_loss: 0.4912 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "3/3 [==============================] - 0s 78ms/step - loss: 0.2859 - accuracy: 0.0000e+00 - val_loss: 0.3827 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - val_loss: 0.1876 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "3/3 [==============================] - 0s 72ms/step - loss: -0.4517 - accuracy: 0.0000e+00 - val_loss: -0.1500 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "3/3 [==============================] - 0s 77ms/step - loss: -1.1610 - accuracy: 0.0000e+00 - val_loss: -0.4692 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "3/3 [==============================] - 0s 74ms/step - loss: -1.8789 - accuracy: 0.0000e+00 - val_loss: -0.7054 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "3/3 [==============================] - 0s 82ms/step - loss: -2.3898 - accuracy: 0.0000e+00 - val_loss: -0.8988 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "3/3 [==============================] - 0s 76ms/step - loss: -2.8632 - accuracy: 0.0000e+00 - val_loss: -1.0705 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: -3.3288 - accuracy: 0.0000e+00 - val_loss: -1.2323 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "3/3 [==============================] - 0s 76ms/step - loss: -3.7658 - accuracy: 0.0000e+00 - val_loss: -1.3902 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "3/3 [==============================] - 0s 79ms/step - loss: -4.1914 - accuracy: 0.0000e+00 - val_loss: -1.5467 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "3/3 [==============================] - 0s 73ms/step - loss: -4.6157 - accuracy: 0.0000e+00 - val_loss: -1.7030 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "3/3 [==============================] - 0s 70ms/step - loss: -5.0249 - accuracy: 0.0000e+00 - val_loss: -1.8594 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "3/3 [==============================] - 0s 79ms/step - loss: -5.5111 - accuracy: 0.0000e+00 - val_loss: -2.0100 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "3/3 [==============================] - 0s 72ms/step - loss: -5.8749 - accuracy: 0.0000e+00 - val_loss: -2.1530 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "3/3 [==============================] - 0s 85ms/step - loss: -6.3281 - accuracy: 0.0000e+00 - val_loss: -2.2913 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "3/3 [==============================] - 0s 78ms/step - loss: -6.6712 - accuracy: 0.0000e+00 - val_loss: -2.4331 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "3/3 [==============================] - 0s 77ms/step - loss: -7.1235 - accuracy: 0.0000e+00 - val_loss: -2.5706 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: -7.5021 - accuracy: 0.0000e+00 - val_loss: -2.7085 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "3/3 [==============================] - 0s 77ms/step - loss: -7.8805 - accuracy: 0.0000e+00 - val_loss: -2.8443 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: -8.2580 - accuracy: 0.0000e+00 - val_loss: -2.9777 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "3/3 [==============================] - 0s 71ms/step - loss: -8.6737 - accuracy: 0.0000e+00 - val_loss: -3.1066 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "3/3 [==============================] - 0s 78ms/step - loss: -9.0094 - accuracy: 0.0000e+00 - val_loss: -3.2366 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: -9.3706 - accuracy: 0.0000e+00 - val_loss: -3.3634 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "3/3 [==============================] - 0s 72ms/step - loss: -9.7706 - accuracy: 0.0000e+00 - val_loss: -3.4851 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "3/3 [==============================] - 0s 75ms/step - loss: -10.1224 - accuracy: 0.0000e+00 - val_loss: -3.6089 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "3/3 [==============================] - 0s 77ms/step - loss: -10.4605 - accuracy: 0.0000e+00 - val_loss: -3.7351 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "3/3 [==============================] - 0s 73ms/step - loss: -10.8222 - accuracy: 0.0000e+00 - val_loss: -3.8600 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "3/3 [==============================] - 0s 76ms/step - loss: -11.1767 - accuracy: 0.0000e+00 - val_loss: -3.9853 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "3/3 [==============================] - 0s 119ms/step - loss: -11.5275 - accuracy: 0.0000e+00 - val_loss: -4.1122 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "3/3 [==============================] - 0s 118ms/step - loss: -11.9074 - accuracy: 0.0000e+00 - val_loss: -4.2397 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "3/3 [==============================] - 0s 120ms/step - loss: -12.3011 - accuracy: 0.0000e+00 - val_loss: -4.3700 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "3/3 [==============================] - 0s 119ms/step - loss: -12.6662 - accuracy: 0.0000e+00 - val_loss: -4.5076 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "3/3 [==============================] - 0s 127ms/step - loss: -13.0641 - accuracy: 0.0000e+00 - val_loss: -4.6477 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "3/3 [==============================] - 0s 124ms/step - loss: -13.4935 - accuracy: 0.0000e+00 - val_loss: -4.7870 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "3/3 [==============================] - 0s 114ms/step - loss: -13.8902 - accuracy: 0.0000e+00 - val_loss: -4.9278 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "3/3 [==============================] - 0s 123ms/step - loss: -14.2741 - accuracy: 0.0000e+00 - val_loss: -5.0697 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "3/3 [==============================] - 0s 119ms/step - loss: -14.6812 - accuracy: 0.0000e+00 - val_loss: -5.2108 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "3/3 [==============================] - 0s 76ms/step - loss: -15.0647 - accuracy: 0.0000e+00 - val_loss: -5.3522 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "3/3 [==============================] - 0s 81ms/step - loss: -15.5106 - accuracy: 0.0000e+00 - val_loss: -5.4891 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "3/3 [==============================] - 0s 81ms/step - loss: -15.9183 - accuracy: 0.0000e+00 - val_loss: -5.6290 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "3/3 [==============================] - 0s 77ms/step - loss: -16.3077 - accuracy: 0.0000e+00 - val_loss: -5.7731 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "3/3 [==============================] - 0s 87ms/step - loss: -16.7228 - accuracy: 0.0000e+00 - val_loss: -5.9175 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "3/3 [==============================] - 0s 76ms/step - loss: -17.1348 - accuracy: 0.0000e+00 - val_loss: -6.0646 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "3/3 [==============================] - 0s 79ms/step - loss: -17.5973 - accuracy: 0.0000e+00 - val_loss: -6.2121 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "3/3 [==============================] - 0s 72ms/step - loss: -17.9813 - accuracy: 0.0000e+00 - val_loss: -6.3654 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "3/3 [==============================] - 0s 79ms/step - loss: -18.3915 - accuracy: 0.0000e+00 - val_loss: -6.5186 - val_accuracy: 0.0000e+00\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 120, 16)           16000     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                6272      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                330       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22613 (88.33 KB)\n",
            "Trainable params: 22613 (88.33 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "         loss  accuracy  val_loss  val_accuracy\n",
            "0    0.651113       0.0  0.646891           0.0\n",
            "1    0.591010       0.0  0.609502           0.0\n",
            "2    0.514061       0.0  0.560624           0.0\n",
            "3    0.424406       0.0  0.491249           0.0\n",
            "4    0.285925       0.0  0.382714           0.0\n",
            "5    0.030543       0.0  0.187629           0.0\n",
            "6   -0.451727       0.0 -0.149951           0.0\n",
            "7   -1.160962       0.0 -0.469211           0.0\n",
            "8   -1.878876       0.0 -0.705394           0.0\n",
            "9   -2.389772       0.0 -0.898795           0.0\n",
            "10  -2.863246       0.0 -1.070546           0.0\n",
            "11  -3.328816       0.0 -1.232263           0.0\n",
            "12  -3.765822       0.0 -1.390211           0.0\n",
            "13  -4.191438       0.0 -1.546682           0.0\n",
            "14  -4.615732       0.0 -1.702954           0.0\n",
            "15  -5.024932       0.0 -1.859414           0.0\n",
            "16  -5.511075       0.0 -2.010007           0.0\n",
            "17  -5.874930       0.0 -2.153013           0.0\n",
            "18  -6.328135       0.0 -2.291267           0.0\n",
            "19  -6.671243       0.0 -2.433058           0.0\n",
            "20  -7.123475       0.0 -2.570576           0.0\n",
            "21  -7.502122       0.0 -2.708549           0.0\n",
            "22  -7.880541       0.0 -2.844282           0.0\n",
            "23  -8.258013       0.0 -2.977730           0.0\n",
            "24  -8.673704       0.0 -3.106597           0.0\n",
            "25  -9.009386       0.0 -3.236609           0.0\n",
            "26  -9.370615       0.0 -3.363419           0.0\n",
            "27  -9.770596       0.0 -3.485055           0.0\n",
            "28 -10.122364       0.0 -3.608917           0.0\n",
            "29 -10.460535       0.0 -3.735090           0.0\n",
            "30 -10.822220       0.0 -3.860043           0.0\n",
            "31 -11.176668       0.0 -3.985270           0.0\n",
            "32 -11.527479       0.0 -4.112177           0.0\n",
            "33 -11.907377       0.0 -4.239722           0.0\n",
            "34 -12.301094       0.0 -4.370031           0.0\n",
            "35 -12.666225       0.0 -4.507582           0.0\n",
            "36 -13.064066       0.0 -4.647705           0.0\n",
            "37 -13.493477       0.0 -4.787040           0.0\n",
            "38 -13.890234       0.0 -4.927845           0.0\n",
            "39 -14.274096       0.0 -5.069698           0.0\n",
            "40 -14.681215       0.0 -5.210833           0.0\n",
            "41 -15.064703       0.0 -5.352179           0.0\n",
            "42 -15.510605       0.0 -5.489148           0.0\n",
            "43 -15.918280       0.0 -5.628964           0.0\n",
            "44 -16.307716       0.0 -5.773080           0.0\n",
            "45 -16.722813       0.0 -5.917497           0.0\n",
            "46 -17.134796       0.0 -6.064602           0.0\n",
            "47 -17.597342       0.0 -6.212061           0.0\n",
            "48 -17.981258       0.0 -6.365387           0.0\n",
            "49 -18.391529       0.0 -6.518630           0.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Assuming data is in a DataFrame called df and the columns are named as per the sample provided\n",
        "# Load data\n",
        "df = pd.read_csv('/content/tweet dataset.csv')  # Correct the file path\n",
        "\n",
        "# Use the correct column name for reviews and labels\n",
        "X = df['text']  # Text column\n",
        "y = df['target']  # Label column, assuming '0' for negative and '1' for positive sentiment\n",
        "\n",
        "# Remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply stopword removal\n",
        "X = X.apply(remove_stopwords)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization and sequence padding\n",
        "vocab_size = 1000\n",
        "oov_tok = \"<OOV>\"\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "sequence_length = 120\n",
        "train_padded = pad_sequences(train_sequences, maxlen=sequence_length, padding='post', truncating='post')\n",
        "test_padded = pad_sequences(test_sequences, maxlen=sequence_length, padding='post', truncating='post')\n",
        "\n",
        "# Model definition\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 16, input_length=sequence_length),\n",
        "    LSTM(32),\n",
        "    Dense(10, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit model with callbacks to avoid overfitting\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3),\n",
        "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "]\n",
        "history = model.fit(train_padded, y_train, epochs=50, validation_data=(test_padded, y_test), callbacks=callbacks)\n",
        "\n",
        "# Output model summary and history\n",
        "print(model.summary())\n",
        "metrics_df = pd.DataFrame(history.history)\n",
        "print(metrics_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_padded, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fASctQEjs69",
        "outputId": "7777ce19-8245-43d7-98fb-14e54f486802"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 35ms/step - loss: -6.5186 - accuracy: 0.0000e+00\n",
            "Test Loss: -6.518629550933838\n",
            "Test Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4v-2c0oqjTY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}